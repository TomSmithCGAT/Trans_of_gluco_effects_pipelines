#############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline project 34 - ChIP-Seq analysis
===========================

:Author: Tom Smith
:Release: $Id$
:Date: |today|
:Tags: Python


Overview
========

This pipeline performs the ChIP-Seq analysis, downstream of mapping and peak calling. This includes:

1. Intersecting peaks
2. MMDiff analysis of differential peak shape
3. Plotting

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_project_34.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import glob
import itertools
import pylab as plt
import numpy as np
import pandas as pd
from matplotlib_venn import venn3, venn2

import CGAT.BamTools as BamTools
import CGAT.Experiment as E
import CGAT.IOTools as IOTools
import CGATPipelines.Pipeline as P
import CGAT.GTF as GTF

import CGATPipelines.PipelineGeneset as PipelineGeneset
import CGATPipelines.PipelineTracks as PipelineTracks
import CGATPipelines.PipelineMappingQC as PipelineMappingQC
import PipelineProject34ChipSeq as P34ChipSeq


# Pipeline configuration
P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"],
    defaults={
        'paired_end': False})

PARAMS = P.PARAMS

# Add parameters from the annotation pipeline, but
# only the interface
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    prefix="annotations_",
    update_interface=True,
    restrict_interface=True))

PipelineGeneset.PARAMS = PARAMS
PipelineMappingQC.PARAMS = PARAMS

# Helper functions mapping tracks to conditions, etc
# determine the location of the input files (reads).
try:
    PARAMS["input"]
except NameError:
    DATADIR = "."
else:
    if PARAMS["input"] == 0:
        DATADIR = "."
    elif PARAMS["input"] == 1:
        DATADIR = "data.dir"
    else:
        DATADIR = PARAMS["input"]  # not recommended practise.

Sample = PipelineTracks.AutoSample

# collect sra nd fastq.gz tracks
TRACKS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("bams/*.call.bam"), "(\S+).call.bam")

GENESETS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("*.gtf.gz"), "(\S+).gtf.gz")

COUNTS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("*.counts.tsv.gz"), "(\S+).counts.tsv.gz")

DESIGNS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("*.design.tsv"), "(\S+).design.tsv")

MACS2PEAKS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("macs2.dir/*.macs2_peaks.xls.gz"),
    "(\S+).macs2_peaks.xls.gz")

RANGERPEAKS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("peakranger.ranger.dir/*.peakranger_region.bed"),
    "(\S+).peakranger_region.bed")

CCATPEAKS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("peakranger.ccat.dir/*.ccat_region.bed"),
    "(\S+).ccat_region.bed")

SICERPEAKS = glob.glob("sicer.*.dir/*.*.sicer.dir/foreground-*-islands-summary-FDR0.050000")

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


def getNreadsDict():
    ''' make a dictionary of sample: # reads '''
    nreads_dir = "/ifs/projects/proj034/ChIP_Seq/full/mapping/nreads.dir/"

    nreads_dict = {}

    for sample in glob.glob("%s/*.nreads" % nreads_dir):
        with IOTools.openFile(sample) as inf:
            nreads = inf.next().strip().split("\t")[1]
            sample = os.path.basename(sample).replace(".nreads", "").split("-")
            sample = "-".join((sample[1], sample[0], sample[2]))
        nreads_dict[sample] = nreads

    return nreads_dict

######################################################################
# Get coverage bigwigs
######################################################################


@mkdir("coverage.dir")
@originate("coverage.dir/sizes.tsv")
def getSizes(outfile):
    ''' compute contig sizes '''
    infile = os.path.join(PARAMS["genome_dir"], PARAMS["genome"] + ".fasta")
    statement = ''' faSize -detailed %(infile)s > %(outfile)s '''
    P.run()


@mkdir("coverage.dir")
@transform("bwa.dir/*.bwa.bam",
           regex("bwa.dir/(\S+).bwa.bam"),
           r"coverage.dir/\1_coverage.bed")
def getCoverage(infile, outfile):
    ''' compute coverage from bams'''

    statement = '''
    bedtools genomecov -bg -ibam %(infile)s > %(outfile)s
    '''

    job_memory = "4G"

    P.run()


def filterPeakrangerBed(infile, outfile):
    '''remove peaks that don't pass the filter
    if no peaks remain, add a dummy peak
    '''

    tmp_file = P.getTempFilename(shared=True)

    n = 0
    with IOTools.openFile(tmp_file, "w") as tmpf:
        with IOTools.openFile(infile, "r") as inf:
            for line in inf:
                if not line.startswith("#"):
                    if "fdrPassed" in line:
                        n += 1
                        tmpf.write(line)

    if n > 0:
        statement = '''sortBed -i %(tmp_file)s |
                       gzip > %(outfile)s;
                       rm -rf %(tmp_file)s'''
        P.run()

    # if no peaks found which pass, add a dummy peak
    else:
        with IOTools.openFile(outfile, "w") as outf:
            outf.write("\t".join(("chrNull", "1", "2", ".", "0")) + "\n")
            os.unlink(tmp_file)


def filterSicerPeaks(infile, outfile, threshold=5.0):
    '''remove peaks that don't pass the threshold fold enrichment)
    if no peaks remain, add a dummy peak
    '''

    n = 0

    with IOTools.openFile(outfile, "w") as outf:
        with IOTools.openFile(infile, "r") as inf:
            for line in inf:
                line = line.strip().split("\t")
                if float(line[6]) >= threshold:
                    n += 1
                    outf.write("%s\n" % "\t".join(line[0:4]))

        if n == 0:
            outf.write("%s\n" % "\t".join(("chrNull", "1", "2", "0")))


######################################################################
# Meta-profiles
######################################################################


@follows(mkdir("gene_profiles.dir"))
@files([(("bams/%s.call.bam" % x.asFile(), "%s.gtf.gz" % y.asFile()),
         ("gene_profiles.dir/%s_%s.tsv.gz" % (x.asFile(), y.asFile())))
        for x, y in itertools.product(TRACKS, GENESETS)])
def buildGeneProfiles(infiles, outfile):

    to_cluster = True

    bamfile, gtffile = infiles
    track = P.snip(bamfile, '.bam')

    statement = '''python %(scriptsdir)s/bam2geneprofile.py
                      --output-filename-pattern="%(outfile)s.%%s"
                      --reporter=transcript
                      --method=geneprofile
                      --method=utrprofile
                      --method=tssprofile
                      --method=separateexonprofilewithintrons
                      --normalize-transcript=total-max
                      --normalize-profile=area
                      %(bamfile)s %(gtffile)s
                   > %(outfile)s
                '''
    P.run()


@merge(buildGeneProfiles,
       "gene_profiles.dir/plots.log")
def plotGeneProfiles(infiles, outfile):
    ''' bame2geneprofile generates plots for each sample. Here we make
    plots with multiple samples '''

    P34ChipSeq.plotGeneProfiles(infiles, outfile)


######################################################################
# prepare bed files
######################################################################

@follows(mkdir("bed.dir"))
@transform([("macs2.dir/%s.macs2_peaks.xls.gz" % x.asFile())
            for x in MACS2PEAKS],
           regex("macs2.dir/(\S+).macs2_peaks.xls.gz"),
           r"bed.dir/\1_macs2_peaks.bed.gz")
def convertMacs2Xls2bed(infile, outfile):
    '''convert xls file to bed file
    Note: xls file may not contain any peaks.
    If this is the case, create a single dummy peak
    "chrNull    1    2    .    0"
    dummy peaks will need to be identified prior to intersection
    '''

    empty = 1
    with IOTools.openFile(infile, "r") as inf:
        for line in inf:
            if not line.startswith("#"):
                empty = 0
                break
    if empty:
        with IOTools.openFile(outfile, "w") as outf:
            outf.write("\t".join(("chrNull", "1", "2", ".", "0")) + "\n")

        outf.close()

    else:
        job_memory = "0.5G"
        statement = """zcat %(infile)s|
                       grep -Ev '(#|start)'|
                       awk -F"\\t" -v OFS="\\t" '{print $1,$2,$3,".",$7}'|
                       gzip > %(outfile)s"""
        P.run()


@follows(mkdir("bed.dir"))
@transform([("peakranger.ccat.dir/%s.ccat_region.bed" % x.asFile())
            for x in CCATPEAKS],
           regex("peakranger.ccat.dir/(\S+).ccat_region.bed"),
           r"bed.dir/\1.ccat_region.bed.gz")
def filterCcatBed(infile, outfile):
    filterPeakrangerBed(infile, outfile)


@follows(mkdir("bed.dir"))
@transform([("peakranger.ranger.dir/%s.peakranger_region.bed" % x.asFile())
            for x in RANGERPEAKS],
           regex("peakranger.ranger.dir/(\S+).peakranger_region.bed"),
           r"bed.dir/\1.ranger_region.bed.gz")
def filterRangerBed(infile, outfile):
    filterPeakrangerBed(infile, outfile)


@follows(mkdir("bed.dir"))
@transform(SICERPEAKS,
           regex("sicer.*.dir/(\S+)\.(\S+).sicer.dir/foreground-.*-FDR0.050000"),
           r"bed.dir/\1_sicer_\2.bed.gz")
def filterSicerBed(infile, outfile):
    filterSicerPeaks(infile, outfile, threshold=2.0)


@follows(mkdir("bed.dir"))
@transform(SICERPEAKS,
           regex("sicer.*.dir/(\S+)\.(\S+).sicer.dir/foreground-.*-FDR0.050000"),
           r"bed.dir/\1_sicer_\2_high_confidence.bed.gz")
def filterSicerHighConfidenceBed(infile, outfile):
    filterSicerPeaks(infile, outfile, threshold=5.0)


######################################################################
# Intersect beds
######################################################################

@mkdir("intersections.dir")
@collate((filterSicerBed, filterSicerHighConfidenceBed),
         regex("bed.dir/(\S+)-(\S+)-(\d+).(\S+).bed.gz$"),
         [r"intersections.dir/\1-\2_\4_peak_intersection.bed.gz",
          r"intersections.dir/\1-\2_\4_peak_all.bed.gz",
          r"intersections.dir/\1-\2_\4_peak_venn_counts.png",
          r"intersections.dir/\1-\2_\4_peak_venn_fraction.png"])
def intersectFilteredBed(infiles, outfiles):
    intersect_outf, peaks_outf, venn_counts, venn_fraction = outfiles

    samples = [os.path.basename(x).split(".")[0] for x in infiles]

    P34ChipSeq.intersectPeaks(infiles, samples, intersect_outf, peaks_outf,
                              venn_counts, venn_fraction)


@mkdir("intersections.dir")
@collate(filterSicerHighConfidenceBed,
         regex("bed.dir/(\S+)-(\S+)-(\d)_sicer_(\S+).bed.gz$"),
         r"intersections.dir/\2_\4_peak_all.bed.gz")
def intersectSicerPeaksHisoneMark(infiles, outfile):
    ''' Get all peaks from all samples for a single mark'''

    samples = [os.path.basename(os.path.dirname(x)).split(".")[0]
               for x in infiles]

    infiles = " ".join(infiles)
    tmp = P.getTempFilename()

    statement = '''zcat %(infiles)s| sort -k1,1 -k2,2n > %(tmp)s; checkpoint;
    bedtools merge -d 100 -i %(tmp)s | gzip > %(outfile)s; checkpoint;
    rm -rf %(tmp)s'''

    P.run()


@mkdir("intersections.dir")
@merge(filterSicerHighConfidenceBed,
       "intersections.dir/all_peak_all.bed.gz")
def intersectSicerPeaksAll(infiles, outfile):
    ''' Get all peaks from all samples for a all hisatone marks'''

    infiles = " ".join(infiles)
    tmp = P.getTempFilename()

    statement = '''zcat %(infiles)s| sort -k1,1 -k2,2n > %(tmp)s; checkpoint;
    bedtools merge -d 100 -i %(tmp)s | gzip > %(outfile)s; checkpoint;
    rm -rf %(tmp)s'''

    P.run()


@mkdir("intersections.dir")
@collate((filterSicerHighConfidenceBed, filterSicerBed),
         regex("bed.dir/(\S+)_sicer_(\S+).bed.gz"),
         [r"intersections.dir/all_\2_peaks_intersect_heatmap.tsv",
          r"intersections.dir/all_\2_peaks_intersect_heatmap.png"])
def intersectSicerPeaksAllheatmap(infiles, outfiles):

    outfile, plotfile = outfiles

    P34ChipSeq.intersectPeaksHeatmap(infiles, outfile, plotfile)


@follows(mkdir("intersections.dir"),
         intersectFilteredBed,
         intersectSicerPeaksHisoneMark,
         intersectSicerPeaksAll,
         intersectSicerPeaksAllheatmap)
def intersect():
    pass

######################################################################
#  MMDiff
######################################################################


@collate(convertMacs2Xls2bed,
         regex("bed.dir/(\S+)-(\S+)-(\d+)_macs2_peaks.bed.gz$"),
         [r"MMDiff.dir/\2_macs2_mmdiff.tsv",
          r"MMDiff.dir/\2_macs2_mmdiff.png"])
def runMMDiffMacs2(infiles, outfiles):
    samples = [P.snip(os.path.basename(x), "_macs2_peaks.bed.gz")
               for x in infiles]

    tsv_outfile, plot_outfile = outfiles

    job_memory = "8G"

    P34ChipSeq.runMMDiff(infiles, samples, tsv_outfile, plot_outfile,
                         submit=False, job_memory=job_memory)


@collate((filterSicerBed, filterSicerHighConfidenceBed),
         regex("bed.dir/(\S+)-(\S+)-(\d+)_sicer_(\S+).bed.gz"),
         [r"MMDiff.dir/\2_sicer_\4_mmdiff.tsv",
          r"MMDiff.dir/\2_sicer_\4_mmdiff.png"])
def runMMDiffSicer(infiles, outfiles):

    samples = [os.path.basename(x).split("_sicer")[0] for x in infiles]

    tsv_outfile, plot_outfile = outfiles

    job_memory = "8G"

    P34ChipSeq.runMMDiff(infiles, samples, tsv_outfile, plot_outfile,
                         submit=True, job_memory=job_memory)


@collate(filterCcatBed,
         regex("bed.dir/(\S+)-(\S+)-(\d+).ccat_region.bed.gz$"),
         [r"MMDiff.dir/\2_ccat_mmdiff.tsv",
          r"MMDiff.dir/\2_ccat_mmdiff.png"])
def runMMDiffCcat(infiles, outfiles):

    samples = [P.snip(os.path.basename(x), ".ccat_region.bed.gz")
               for x in infiles]

    tsv_outfile, plot_outfile = outfiles

    job_memory = "8G"

    P34ChipSeq.runMMDiff(infiles, samples, tsv_outfile, plot_outfile,
                         submit=False, job_memory=job_memory)

@collate(filterRangerBed,
         regex("bed.dir/(\S+)-(\S+)-(\d+).ranger_region.bed.gz$"),
         [r"MMDiff.dir/\2_ranger_mmdiff.tsv",
          r"MMDiff.dir/\2_ranger_mmdiff.png"])
def runMMDiffRanger(infiles, outfiles):

    samples = [P.snip(os.path.basename(x), ".ranger_region.bed.gz")
               for x in infiles]

    tsv_outfile, plot_outfile = outfiles

    job_memory = "8G"

    P34ChipSeq.runMMDiff(infiles, samples, tsv_outfile, plot_outfile,
                         submit=False, job_memory=job_memory)

    
@follows(mkdir("MMDiff.dir"),
         runMMDiffSicer)
def MMDiff():
    pass

######################################################################
#  Enrichment analysis
######################################################################
# This section takes the context_stats database table from pipeline_mapping
# and performs various analyses to identify whether there are any signficant
# enrichments

# to do: paramaterise the variables below
mapping_database = "/ifs/projects/proj034/ChIP_Seq/full/mapping/csvdb"

tracks_control = "H3-"

mapper = "bwa"

Sample = PipelineTracks.AutoSample
Sample.attributes = ('mark', 'condition', 'replicate')

CONTROLTRACKS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    [x for x in glob.glob(
        "/ifs/projects/proj034/ChIP_Seq/full/mapping/bwa.dir/*.bam")
     if tracks_control in x], "(\S+).bwa.bam")

TRACKS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    [x for x in glob.glob(
        "/ifs/projects/proj034/ChIP_Seq/full/mapping/bwa.dir/*.bam")
     if tracks_control not in x and "INPUT" not in x], "(\S+).bwa.bam")

MODIFICATIONS = PipelineTracks.Aggregate(TRACKS, labels=("mark",))

EXPERIMENTS = PipelineTracks.Aggregate(TRACKS,
                                       labels=("mark", "condition"))


@originate("enrichment.dir/enrichment_ttest.tsv")
def enrichmentTTest(outfile):

    P34ChipSeq.enrichmentTTest(mapping_database, CONTROLTRACKS, TRACKS,
                               MODIFICATIONS, mapper, outfile, submit=True)


@originate("enrichment.dir/enrichment_blockedANOVA.tsv")
def enrichmentBlockedANOVA(outfile):

    filename_base = P.snip(outfile, ".tsv")

    P34ChipSeq.enrichmentBlockedANOVA(mapping_database, CONTROLTRACKS,
                                      TRACKS, EXPERIMENTS,
                                      mapper, outfile, filename_base,
                                      submit=True)


@originate("enrichment.dir/enrichment.tsv")
def enrichmentPlots(outfile):

    filename_base = P.snip(outfile, ".tsv")

    P34ChipSeq.enrichmentPlots(mapping_database, CONTROLTRACKS, TRACKS,
                               EXPERIMENTS, mapper, filename_base,
                               submit=True)


@transform([enrichmentTTest,
            enrichmentBlockedANOVA,
            enrichmentPlots],
           suffix(".tsv"),
           ".load")
def loadEnrichments(infile, outfile):
    P.load(infile, outfile)


@follows(mkdir("enrichment.dir"),
         loadEnrichments)
def enrichmentAnalysis():
    pass



######################################################################
# Heatmaps
######################################################################
# We want to plot heatmaps over promoters. First we need a bed file of
# promoter locations

# PARAMATERISE
@mkdir("figures.dir")
@originate("figures.dir/protein_coding_promoters.bed")
def makeProteinCodingPromoterBed(outfile):
    ''' take the protein coding gene coordinates and derive the promoter positions '''

    select_cmd = '''SELECT gene_id, contig, start, end, strand
    FROM geneset_all_gtf_genome_coordinates
    WHERE attributes='gene_biotype "protein_coding"'
    '''

    select = connect().execute(select_cmd)

    with IOTools.openFile(outfile, "w") as outf:

        for entry in select:

            gene, contig, start, stop, strand = entry

            if strand == "+":
                p_start = start - 500
                p_end = start + 500
            elif strand == "-":
                p_start = stop - 500
                p_end = stop + 500

            outf.write("%s\n" % "\t".join(
                map(str, (contig, p_start, p_end, gene, 255, strand))))


@mkdir("figures.dir")
@originate("figures.dir/miRNA_promoters.bed")
def makeMiRNAPromoterBed(outfile):
    ''' take the miRNA gene coordinates and derive the promoter positions '''

    select_cmd = '''SELECT gene_id, contig, start, end, strand
    FROM geneset_all_gtf_genome_coordinates
    WHERE attributes='gene_biotype "miRNA"'
    '''

    select = connect().execute(select_cmd)

    with IOTools.openFile(outfile, "w") as outf:

        for entry in select:

            gene, contig, start, stop, strand = entry

            if strand == "+":
                p_start = start - PARAMS['plots_promoter_pad']
                p_end = start
            elif strand == "-":
                p_start = stop
                p_end = stop + PARAMS['plots_promoter_pad']

            outf.write("%s\n" % "\t".join(
                map(str, (contig, p_start, p_end, gene, 255, strand))))


@mkdir("figures.dir")
@originate("figures.dir/piRNA_promoters.bed")
def makePiRNAPromoterBed(outfile):
    ''' take the piRNA gene coordinates and derive the promoter positions '''

    piRNA_gtf = "/ifs/projects/proj034/sRNA_Seq/full/rnaseqdiffexpression/piRBase_rn5.gtf.gz"
    gtf_entries = GTF.iterator(IOTools.openFile(piRNA_gtf))

    with IOTools.openFile(outfile, "w") as outf:

        for gtf in gtf_entries:

            if gtf.strand == "+":
                p_start = gtf.start - PARAMS['plots_promoter_pad']
                p_end = gtf.start
            elif gtf.strand == "-":
                p_start = gtf.end
                p_end = gtf.end + PARAMS['plots_promoter_pad']

            outf.write("%s\n" % "\t".join(
                map(str, (gtf.contig, p_start, p_end,
                          gtf.asDict()['transcript_id'],
                          255, gtf.strand))))


@transform("bams/*.call.bam",
           regex("bams/(\S+).call.bam"),
           add_inputs(makeProteinCodingPromoterBed),
           r"figures.dir/\1.cov.tsv")
def getProteinCodingPromoterCoverage(infiles, outfile):
    ''' use bedtools to get the per base coverage over the promoters '''

    bam, bed = infiles

    # awk to remove redundant information from bedtools outfile
    statement = '''
    bedtools coverage -d -abam %(bam)s -b %(bed)s |
    awk 'OFS="\t" {print $1, $2, $6, $7}' > %(outfile)s '''

    P.run()


@transform("bams/*.call.bam",
           regex("bams/(\S+)-(H3K\S+)-(\d+).call.bam"),
           add_inputs(r"intersections.dir/\2_narrow_high_confidence_peak_all.bed.gz"),
           r"figures.dir/\1-\2-\3_peaks.cov.tsv")
def getPeakCoverage(infiles, outfile):
    ''' use bedtools to get the per base coverage over Peaks '''

    bam, bed = infiles

    # awk to remove redundant information from bedtools outfile
    statement = '''
    bedtools coverage -d -b %(bam)s -a %(bed)s |
    awk 'OFS="\t" {print $1, $2, $4, $5}' > %(outfile)s '''

    P.run()


@transform("bams/*.call.bam",
           regex("bams/(\S+)-(H3K\S+)-(\d+).call.bam"),
           add_inputs(intersectSicerPeaksAll),
           r"figures.dir/\1-\2-\3_all_peaks.cov.tsv")
def getAllPeakCoverage(infiles, outfile):
    ''' use bedtools to get the per base coverage over the promoters '''

    bam, bed = infiles

    # awk to remove redundant information from bedtools outfile
    statement = '''
    bedtools coverage -d -b %(bam)s -a %(bed)s |
    awk 'OFS="\t" {print $1, $2, $4, $5}' > %(outfile)s '''

    P.run()


@transform([getProteinCodingPromoterCoverage,
            getPeakCoverage,
            getAllPeakCoverage],
           suffix(".cov.tsv"),
           ".cov_binned.tsv")
def getCountsPerWindow(infile, outfile):

    bins = 200

    with IOTools.openFile(outfile, "w") as outf:
        outf.write("%s\n" % "\t".join(["contig", "contig_pos", "peak_pos", "count"]))
        with IOTools.openFile(infile, "r") as inf:

            tmp_counts = []
            last_contig = ""
            last_contig_pos = 0
            n = 0
            for line in inf:
                n+=1
                contig, contig_pos, peak_pos, count = line.strip().split(" ")
                if contig != last_contig or contig_pos != last_contig_pos and n > 1:

                    start = 0
                    end = 0
                    bin_width = float(len(tmp_counts))/bins

                    for bin_ix in range(0, bins):
                        end += bin_width
                        window_average = np.mean(
                                tmp_counts[int(round(start, 0)):int(round(end, 0))])
                        try:
                            outf.write("%s\n" % "\t".join(map(str, (
                                last_contig, last_contig_pos,
                                str(bin_ix+1), str(window_average)))))
                        except TypeError as err:
                            print(n, err)
                            print(line, last_contig, last_contig_pos, str(bin_ix+1), str(window_average))
                            raise ValueError

                        start += bin_width

                    tmp_counts = [int(count)]
                    last_contig = contig
                    last_contig_pos = contig_pos

                else:
                    tmp_counts.append(int(count))
                    last_contig_pos = contig_pos

            start = 0
            end = 0
            bin_width = float(len(tmp_counts))/bins

            for bin_ix in range(0, bins):
                end += bin_width
                window_average = np.mean(
                        tmp_counts[int(round(start, 0)):int(round(end, 0))])
                outf.write("%s\n" % "\t".join([
                    last_contig, last_contig_pos,
                    str(bin_ix+1), str(window_average)]))
                start += bin_width


@transform("bams/*.call.bam",
           regex("bams/(\S+).call.bam"),
           add_inputs(makeMiRNAPromoterBed),
           r"figures.dir/\1_miRNA_cov.tsv")
def getMiRNAPromoterCoverage(infiles, outfile):
    ''' use bedtools to get the coverage over the promoters '''

    bam, bed = infiles

    statement = '''
    bedtools coverage -b %(bam)s -a %(bed)s > %(outfile)s '''

    P.run()


@transform("bams/*.call.bam",
           regex("bams/(\S+).call.bam"),
           add_inputs(makePiRNAPromoterBed),
           r"figures.dir/\1_piRNA_cov.tsv")
def getPiRNAPromoterCoverage(infiles, outfile):
    ''' use bedtools to get the coverage over the promoters '''

    bam, bed = infiles

    statement = '''
    bedtools coverage -b %(bam)s -a %(bed)s > %(outfile)s '''

    P.run()


@mkdir("figures.dir")
@transform("bams/*.call.bam",
           regex("bams/(\S+)-(\S+)-(\d+).call.bam"),
           add_inputs(intersectSicerPeaksAll),
           r"figures.dir/\1-\2-\3_all_peaks.cov.tsv")
def getAllPeakCounts(infiles, outfile):
    ''' use bedtools to get the counts per peak'''


    bam, bed = infiles
    statement = '''
    bedtools coverage -b %(bam)s -a %(bed)s > %(outfile)s '''

    P.run()


@merge(getAllPeakCounts,
       "figures.dir/merged_all_peaks.tsv")
def mergePeakCounts(infiles, outfile):
    ''' merge the peak counts '''

    with IOTools.openFile(outfile, "w") as outf:
        outf.write("%s\n" % "\t".join(
            ["sample", "condition", "mark", "replicate", "contig",
             "start", "length", "count"]))

        for infile in infiles:
            sample = os.path.basename(infile).replace("_all_peaks.cov.tsv", "")
            condition, mark, replicate = sample.split("-")
            with IOTools.openFile(infile, "r") as inf:
                for line in inf:
                    (contig, start, end, count, covered, length,
                     fraction_covered) = line.strip().split("\t")
                    outf.write("%s\n" % "\t".join((
                        sample, condition, mark, replicate,
                        contig, start, length, count)))


@transform(mergePeakCounts,
           suffix(".tsv"),
           "_heatmap.png")
def plotChipSeqheatmap(infile, outfile):
    ''' make a heatmap + dendograms for the ChIP-Seq enrichment scores'''

    nreads_dict = getNreadsDict()

    P34ChipSeq.plotChipSeqheatmap(infile, outfile, nreads_dict)


@collate((getPiRNAPromoterCoverage,
          getMiRNAPromoterCoverage),
         regex("figures.dir/(\S+)_(\S+)_cov.tsv"),
         r"figures.dir/\2_correlation_histone_sRNA.png")
def plotHistonesRNASeqCorrelation(infiles, outfile):
    ''' correlate the sRNA-Seq fold changes and fold change in
    piRNA/miRNA promoter histone enrichment fold changes '''

    nreads_dir = "/ifs/projects/proj034/ChIP_Seq/full/mapping/nreads.dir/"

    DESeq2_dir = "/ifs/projects/proj034/sRNA_Seq/full/project_pipeline/DESeq.dir"

    if "miRNA" in infiles[0]:
        alpha = 1
        suffix = "_miRNA_cov.tsv"
        DESeq2_fold_inf = os.path.join(
            DESeq2_dir,
            "miRNA.feature_counts_bwa_deseq2_F1-Saline_F1-Dex_results.tsv")

    elif "piRNA" in infiles[0]:
        alpha = 0.25
        suffix = "_piRNA_cov.tsv"
        DESeq2_fold_inf = os.path.join(
            DESeq2_dir,
            "piRBase_rn5.feature_counts_bwa_deseq2_F1-Saline_F1-Dex_results.tsv")

    nreads_dict = getNreadsDict()

    P34ChipSeq.plotHistoneRNACorrelation(
        infiles, outfile, nreads_dir, alpha, DESeq2_fold_inf,
        suffix, nreads_dict, PARAMS['plots_promoter_pad'])


@follows(mergePeakCounts,
         plotHistonesRNASeqCorrelation,
         plotChipSeqheatmap)
def coverage():
    pass


######################################################################
#  Generic pipeline tasks
######################################################################

@follows(intersect,
         MMDiff,
         enrichmentAnalysis,
         plotGeneProfiles,
         mergePeakCounts,
         coverage)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
