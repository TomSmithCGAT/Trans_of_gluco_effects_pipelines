##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline Proj034 sRNA
===========================

:Author: Tom Smith
:Release: $Id$
:Date: |today|
:Tags: Python

Overview
========

This pipeline performs the analysis of the sRNA-Seq data including:

1. Iterative mapping
2. "Spike-in" in-silico power analysis
3. Plotting

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_project_34.py config

Input files
-----------

Soft links were made to the output of the readqc pipeline for trimmed fastqs, e.g:
F2-Saline-4.fastq.gz -> ../readqc/processed.dir/trimmomatic-F2-Saline-4.fastq.gz

+ the BAMs (and indexes) from the output of the mapping pipeline, e.g:
F1-Saline-1-bwa.bam -> ../mapping/bwa.dir/F1-Saline-1.bwa.bam

+ the gtfs used in the rnaseqdiffexpression pipeline, e.g:
tRNA_no_duplicate_transcripts.gtf.gz -> ../rnaseqdiffexpression/tRNA_no_duplicate_transcripts.gtf.gz

+ the counts tables from the rnaseqdiffexpression pipeline, e.g:
tRNA_no_duplicates.feature_counts_butter.counts.tsv.gz -> ../rnaseqdiffexpression/feature_counts.dir/tRNA_no_duplicates.feature_counts.tsv.gz

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

Requirements:

* samtools >= 1.1
* bowtie2

Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import glob
import itertools
from rpy2.robjects import r as R
import rpy2.robjects as ro
import pandas.rpy.common as com
import pandas as pd
import numpy as np
import collections
import pysam

import CGAT.Experiment as E
import CGAT.IOTools as IOTools
import CGATPipelines.Pipeline as P
import CGAT.Expression as Expression
import CGAT.Counts as Counts
import CGAT.FastaIterator as FastaIterator
import CGATPipelines.PipelineGeneset as PipelineGeneset
import CGATPipelines.PipelineTracks as PipelineTracks
import CGATPipelines.PipelineMapping as PipelineMapping
import CGATPipelines.PipelineMappingQC as PipelineMappingQC

import PipelineProject34sRNA as P34

# Pipeline configuration
P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"],
    defaults={
        'paired_end': False})

PARAMS = P.PARAMS

PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    prefix="annotations_",
    update_interface=True,
    restrict_interface=True))


PipelineGeneset.PARAMS = PARAMS
PipelineMappingQC.PARAMS = PARAMS

# Helper functions mapping tracks to conditions, etc
# determine the location of the input files (reads).
try:
    PARAMS["input"]
except NameError:
    DATADIR = "."
else:
    if PARAMS["input"] == 0:
        DATADIR = "."
    elif PARAMS["input"] == 1:
        DATADIR = "data.dir"
    else:
        DATADIR = PARAMS["input"]  # not recommended practise.

Sample = PipelineTracks.AutoSample

# collect bam tracks
TRACKS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("*.bam"), "(\S+).bam")

# collect fastq.gz tracks
SEQUENCEFILES = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("*.fastq.gz"), "(\S+).fastq.gz")

GENESETS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("*.gtf.gz"), "(\S+).gtf.gz")

COUNTS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("*.counts.tsv.gz"), "(\S+).counts.tsv.gz")

DESIGNS = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob("*.design.tsv"), "(\S+).design.tsv")


###############################################################################
# Utility functions
###############################################################################

def connect():
    '''connect to database.

    This method also attaches to helper databases.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])

    if not os.path.exists(PARAMS["annotations_database"]):
        raise ValueError(
            "can't find database '%s'" %
            PARAMS["annotations_database"])

    statement = '''ATTACH DATABASE '%s' as annotations''' % \
                (PARAMS["annotations_database"])

    cc = dbh.cursor()
    cc.execute(statement)

    statement = '''ATTACH DATABASE '%s' as mapping''' % \
                ("/ifs/projects/proj034/sRNA_Seq/full/mapping/csvdb")

    cc.execute(statement)

    statement = '''ATTACH DATABASE '%s' as rnaseqdiff''' % \
                ("/ifs/projects/proj034/sRNA_Seq/full/rnaseqdiffexpression_bwa/csvdb")

    cc.execute(statement)

    statement = '''ATTACH DATABASE '%s' as readqc''' % \
                ("/ifs/projects/proj034/sRNA_Seq/full/readqc/csvdb")

    cc.execute(statement)

    cc.close()

    return dbh


# README from GAPP_DIR below:
#This directory contains a re-analysis of the data from Gapp et al 2014
#as a validation of our methods
#http://www.nature.com/neuro/journal/v17/n5/full/nn.3695.html

# re-analysis of gapp et al data
GAPP_DIR = "/ifs/projects/proj034/sRNA_Seq/gapp/readqc/processed.dir/"
GAPP_SEQUENCEFILES = PipelineTracks.Tracks(Sample).loadFromDirectory(
    glob.glob(os.path.join(GAPP_DIR, "trimmed-*.fastq.gz")),
    "(\S+).fastq.gz")

###############################################################################
# Iterative mapping - gapp et al - collapse fastqs
###############################################################################


@mkdir("gapp.dir/iterative_mapping.dir/collapsed_fastqs.dir")
@transform(["%s/%s.fastq.gz" % (GAPP_DIR, x.asFile())
            for x in GAPP_SEQUENCEFILES],
           regex("(\S+)/trimmed-(\S+).fastq.gz"),
           r"gapp.dir/iterative_mapping.dir/collapsed_fastqs.dir/\2.fastq")
def collapseFastqGapp(infile, outfile):
    ''' collapse fastqs into unique reads with counts appended to identifier'''

    collapse_log = P.snip(outfile, ".fastq") + "_collapse.log"

    P34.collapseFastq(infile, outfile,  collapse_log,
                      submit=True, job_memory="10G")

###############################################################################
# Iterative mapping - gapp et al - create fastas
###############################################################################


@mkdir("gapp.dir/iterative_mapping.dir/annotations.dir")
@transform("/ifs/projects/proj034/data/miRNA/mm9_miRBase_no_duplicates.gtf.gz",
           regex("(\S+).gtf.gz"),
           r"iterative_mapping.dir/annotations.dir/miRNA.fa")
def createFastasFromGTFsGapp(infile, outfile):
    ''' create fasta file from gtfs and index with bowtie'''

    assert os.path.exists(infile), E.warn("file: %s does not exist" % infile)

    bowtie_index = P.snip(outfile, ".fa")
    bowtie_log = "%s_bowtie_build.log" % bowtie_index

    genome_fasta = os.path.join(PARAMS["genome_dir"], "mm9.fasta")

    statement = '''
    zcat %(infile)s|
    python %(scriptsdir)s/gff2fasta.py -v10 --is-gtf --feature=exon
    --genome-file=%(genome_fasta)s -L %(outfile)s.log 
    > %(outfile)s;
    samtools faidx %(outfile)s;
    bowtie-build %(outfile)s %(bowtie_index)s > %(bowtie_log)s'''

    P.run()


###############################################################################
# Iterative mapping - gapp et al - Bowtie mapping wrapper
###############################################################################

@transform(collapseFastqGapp,
           regex("gapp.dir/iterative_mapping.dir/collapsed_fastqs.dir/(\S+).fastq"),
           add_inputs(createFastasFromGTFsGapp),
           r"gapp.dir/iterative_mapping.dir/\1_counts.tsv")
def iterativeBowtieMappingGapp(infiles, outfile):
    ''' perform iterative mapping using bowtie
    alignment is performed in the order specified in mapping_order param
    Initially only uniquely aligning reads are included'''

    infile = infiles[0]
    fastas = infiles[1]

    P34.iterativeBowtieMapping(
        infile, fastas, outfile,
        mismatches=0,
        seed_length=15,
        mapping_order="miRNA",
        multimapping_max=10)


###############################################################################
# Iterative mapping - gapp et al - Quantification - miRNAs
###############################################################################
# max counts per loci (mature miRNA)

@merge(iterativeBowtieMappingGapp,
       "iterative_mapping.dir/counts_mature_miRNA.tsv")
def mergeReadsPerMaturemiRNAGapp(infiles, outfile):
    ''' merge the counts per tRF into a single table for DE analysis '''
    df = pd.DataFrame()

    for infile in infiles:
        sample = P.snip(os.path.basename(infile), "_counts.tsv")

        # we actually want the coverage on the forward strand
        infile = infile.replace("_counts.tsv", "_cov_forward.tsv")

        tmp_df = pd.read_table(infile, sep="\t", header=None,
                               usecols=[0, 1, 2], index_col=0)

        tmp_df = tmp_df.ix[tmp_df.index == "miRNA"]

        df[sample] = [max(map(int, x.split(","))) for x in tmp_df.ix[:, 2]]

    df.set_index(tmp_df.ix[:, 1], inplace=True)

    df.to_csv(outfile, sep="\t", index=True)


@follows(mergeReadsPerMaturemiRNAGapp)
def gapp():
    pass
###############################################################################
# Iterative mapping - collapse fastqs
###############################################################################


@mkdir("iterative_mapping.dir/collapsed_fastqs.dir")
@transform(["%s.fastq.gz" % x.asFile() for x in SEQUENCEFILES],
           regex("(\S+).fastq.gz"),
           r"iterative_mapping.dir/collapsed_fastqs.dir/\1.fastq")
def collapseFastq(infile, outfile):
    ''' collapse fastqs into unique reads with counts appended to identifier'''

    collapse_log = P.snip(outfile, ".fastq") + "_collapse.log"

    P34.collapseFastq(infile, outfile,  collapse_log,
                      submit=True, job_memory="4G")


###############################################################################
# Iterative mapping - create fastas
###############################################################################


@originate(["iterative_mapping.dir/annotations.dir/%s.fa" % x
            for x in PARAMS["iterative_mapping_repeats"].split(",")])
def createRepeatFastas(outfile):
    ''' create fasta file for each repeat class and index with bowtie'''
    tmpfile = P.getTempFilename("/ifs/scratch/")

    bowtie_index = P.snip(outfile, ".fa")
    bowtie_log = "%s_bowtie_build.log" % bowtie_index
    repeat = os.path.basename(bowtie_index)

    P34.importRNAAnnotationFromUCSC(tmpfile, (repeat,), PARAMS["genome"])

    genome_fasta = os.path.join(PARAMS["genome_dir"],
                                PARAMS["genome"] + ".fasta")

    statement = '''
    sed 's/exon/CDS/g' %(tmpfile)s|
    python %(scriptsdir)s/gtf2gtf.py  --method=rename-duplicates
    --duplicate-feature=transcript|
    sed 's/CDS/exon/g' |
    python %(scriptsdir)s/gff2fasta.py -v0 --is-gtf --feature=exon
    --genome-file=%(genome_fasta)s > %(outfile)s;
    samtools faidx %(outfile)s;
    bowtie-build %(outfile)s %(bowtie_index)s > %(bowtie_log)s'''

    P.run()
    os.unlink(tmpfile)


@originate("iterative_mapping.dir/annotations.dir/other_repeat.fa")
def createCombinedRepeatsFastas(outfile):
    ''' create combined fasta file for repeat classes and index with bowtie'''
    tmpfile = P.getTempFilename("/ifs/scratch/")
    tmpfile = P.getTempFilename(".")

    bowtie_index = P.snip(outfile, ".fa")
    bowtie_log = "%s_bowtie_build.log" % bowtie_index

    repeats = list(PARAMS["iterative_mapping_combined_repeats"].split(","))

    P34.importRNAAnnotationFromUCSC(tmpfile, repeats, PARAMS["genome"])

    genome_fasta = os.path.join(PARAMS["genome_dir"],
                                PARAMS["genome"] + ".fasta")

    statement = '''
    sed 's/exon/CDS/g' %(tmpfile)s|
    python %(scriptsdir)s/gtf2gtf.py  --method=rename-duplicates
    --duplicate-feature=transcript|
    sed 's/CDS/exon/g' |
    python %(scriptsdir)s/gff2fasta.py -v0 --is-gtf --feature=exon
    --genome-file=%(genome_fasta)s > %(outfile)s;
    samtools faidx %(outfile)s;
    bowtie-build %(outfile)s %(bowtie_index)s > %(bowtie_log)s'''

    P.run()
    os.unlink(tmpfile)


@follows(mkdir("iterative_mapping.dir/annotations.dir"))
@transform(["%s.gtf.gz" % x
            for x in PARAMS["iterative_mapping_gtfs"].split(",")],
           regex("(\S+).gtf.gz"),
           r"iterative_mapping.dir/annotations.dir/\1.fa")
def createFastasFromGTFs(infile, outfile):
    ''' create fasta file from gtfs and index with bowtie'''

    assert os.path.exists(infile), E.warn("file: %s does not exist" % infile)

    bowtie_index = P.snip(outfile, ".fa")
    bowtie_log = "%s_bowtie_build.log" % bowtie_index

    genome_fasta = os.path.join(PARAMS["genome_dir"],
                                PARAMS["genome"] + ".fasta")

    statement = '''
    zcat %(infile)s|
    sed 's/exon/CDS/g' |
    python %(scriptsdir)s/gtf2gtf.py  --method=rename-duplicates
    --duplicate-feature=transcript|
    sed 's/CDS/exon/g' |
    python %(scriptsdir)s/gff2fasta.py -v10 --is-gtf --feature=exon
    --genome-file=%(genome_fasta)s -L %(outfile)s.log 
    > %(outfile)s;
    samtools faidx %(outfile)s;
    bowtie-build %(outfile)s %(bowtie_index)s > %(bowtie_log)s'''

    P.run()


@originate("iterative_mapping.dir/annotations.dir/miRNA.fa")
def createmiRNAFasta(outfile):
    '''create miRNA fasta from miRBase mature miRNA and index with bowtie'''

    # infile is hardcoded!
    infile = "/ifs/projects/proj034/data/miRNA/miRBase_mature_miRNA.fa.gz"

    FAiterator = FastaIterator.iterate(IOTools.openFile(infile))

    def RNA2DNA(string):
        return "".join(["T" if x == "U" else x for x in string.upper()])

    with IOTools.openFile(outfile, "w") as outf:
        for entry in FAiterator:
            if "Rattus norvegicus" in entry.title:
                entry.sequence = RNA2DNA(entry.sequence)
                outf.write("%s\n" % str(entry))

    bowtie_index = P.snip(outfile, ".fa")
    bowtie_log = "%s_bowtie_build.log" % bowtie_index

    statement = '''
    bowtie-build %(outfile)s %(bowtie_index)s > %(bowtie_log)s'''

    P.run()


@originate(["iterative_mapping.dir/annotations.dir/%s.fa" % x
            for x in PARAMS["iterative_mapping_annotations"].split(",")])
def createFastas(outfile):
    ''' create fasta file for each annotation class and index with bowtie'''
    bowtie_index = P.snip(outfile, ".fa")
    bowtie_log = "%s_bowtie_build.log" % bowtie_index
    annotation = os.path.basename(bowtie_index)

    # note: the name of this parameter has changed in the most recent
    # pipeline_annotations to annotations_interface_gene_info
    table = PARAMS["annotations_interface_table_gene_info"]

    select_cmd = '''SELECT DISTINCT gene_id FROM gene_info
                    WHERE gene_biotype IS '%(annotation)s';''' % locals()
    dbh = connect()

    try:
        select = dbh.execute(select_cmd)
    except sqlite3.OperationalError as error:
        E.critical("sqlite3 cannot find table. Error message: '%s'" % error)

    tmpfile = P.getTempFilename("/ifs/scratch/")

    with IOTools.openFile(tmpfile, "w") as outf:
        outf.write("\n".join(("\t".join(x) for x in select)) + "\n")

    geneset_all = PARAMS["annotations_interface_geneset_all_gtf"]
    genome_fasta = os.path.join(PARAMS["genome_dir"],
                                PARAMS["genome"] + ".fasta")

    statement = '''
    zcat %(geneset_all)s |
    python %(scriptsdir)s/gtf2gtf.py --method=filter --filter-method=gene
    --map-tsv-file=%(tmpfile)s |
    python %(scriptsdir)s/gff2fasta.py -v0 --is-gtf --feature=exon
    --genome-file=%(genome_fasta)s > %(outfile)s;
    samtools faidx %(outfile)s;
    bowtie-build %(outfile)s %(bowtie_index)s > %(bowtie_log)s'''

    P.run()
    os.unlink(tmpfile)


###############################################################################
# Iterative mapping - Bowtie mapping wrapper
###############################################################################

@transform(collapseFastq,
           regex("iterative_mapping.dir/collapsed_fastqs.dir/(\S+).fastq"),
           add_inputs(createFastas,
                      createmiRNAFasta,
                      createRepeatFastas,
                      createFastasFromGTFs,
                      createCombinedRepeatsFastas),
           r"iterative_mapping.dir/\1_counts.tsv")
def iterativeBowtieMapping(infiles, outfile):
    ''' perform iterative mapping using bowtie
    alignment is performed in the order specified in mapping_order param
    Initially only uniquely aligning reads are included'''

    infile = infiles[0]
    fastas = infiles[1:]

    P34.iterativeBowtieMapping(
        infile, fastas, outfile,
        mismatches=PARAMS['iterative_mapping_mismatches'],
        seed_length=PARAMS['iterative_mapping_seed_length'],
        mapping_order=PARAMS["iterative_mapping_order"],
        multimapping_max=PARAMS["iterative_mapping_multimapping_max"])

###############################################################################
# Iterative mapping - Quantification and profile plots - tRNAs
###############################################################################


@follows(mkdir("iterative_mapping.dir/plots.dir"))
@transform(iterativeBowtieMapping,
           regex("iterative_mapping.dir/(\S+)_counts.tsv"),
           [r"iterative_mapping.dir/plots.dir/\1_tRNA_profile_sum.tsv",
            r"iterative_mapping.dir/plots.dir/\1_tRNA_profile_normalised.tsv"])
def plottRNAProfile(infile, outfiles):
    ''' plot coverage profile across tRNA loci'''
    #for x in PARAMS:
    #    if "iterative" in x:
    #        print x, PARAMS[x]

    length = PARAMS['iterative_mapping_trna_length']

    # we actually want the forward coverage file here
    infile = infile.replace("_counts.tsv", "_cov_forward.tsv")

    P34.plottRNAProfile(infile, outfiles, length)


@merge(plottRNAProfile,
       ["iterative_mapping.dir/plots.dir/tRNA_profile_sum.tsv",
        "iterative_mapping.dir/plots.dir/tRNA_profile_normalised.tsv"])
def mergetRNAProfiles(infiles, outfiles):
    ''' merge the tRNA coverage profiles into single tables for comparison'''

    def normalise(array):
        max_array = max(array)
        return [x / max_array for x in array]

    sum_outfile, norm_outfile = outfiles

    df_sum = pd.DataFrame()
    df_norm = pd.DataFrame()

    for sample_infiles in infiles:
        sample = P.snip(os.path.basename(sample_infiles[0]),
                        "_tRNA_profile_sum.tsv")

        tmp_df_sum = pd.read_table(sample_infiles[0])
        tmp_df_sum['mean'] = normalise(tmp_df_sum['mean'])
        tmp_df_sum["sample"] = sample

        tmp_df_norm = pd.read_table(sample_infiles[1])
        tmp_df_norm['mean'] = normalise(tmp_df_norm['mean'])
        tmp_df_norm["sample"] = sample

        df_sum = df_sum.append(tmp_df_sum)
        df_norm = df_norm.append(tmp_df_norm)

    df_sum.to_csv(sum_outfile, sep="\t", index=False)
    df_norm.to_csv(norm_outfile, sep="\t", index=False)

    df_sum["id_2"] = [x for x in df_sum["id_2"]]

    plotProfile = R('''
    function(df, plot_outfile){
    library(ggplot2)

    df$id_1 = sapply(strsplit(df$sample, "-"), "[", 1)
    df$id_2 = sapply(strsplit(df$sample, "-"), "[", 2)
    df$id_3 = sapply(strsplit(df$sample, "-"), "[", 3)

    l_txt = element_text(size=20)

    p = ggplot(df[df$id_1=="F1",],
               aes(x=as.numeric(nt), y=as.numeric(mean),
                   group=sample, colour=id_2)) +
    geom_point(size=2) + geom_line() +
    scale_colour_discrete(name="Sample") +
    theme_bw() +
    theme(
      aspect.ratio = 1,
      axis.text.x = l_txt,
      axis.title.y = l_txt,
      axis.title.x = l_txt,
      axis.text.y = l_txt,
      legend.text = l_txt,
      legend.title = l_txt) +
    xlab("Distance from 5' end (bp)") +
    ylab("Coverage") +
    scale_y_continuous(limits=c(0,1), breaks=seq(0,1,0.2)) +
    scale_x_continuous(limits=c(0,40), breaks=seq(0,40,5))

    ggsave(plot_outfile, width=10, height=10)
    }''')

    # need to reset index as duplicate row names are not allowed
    df_sum.set_index([range(0, len(df_sum.index))], inplace=True)
    df_norm.set_index([range(0, len(df_norm.index))], inplace=True)

    r_df_sum = com.convert_to_r_dataframe(df_sum)
    r_df_norm = com.convert_to_r_dataframe(df_norm)

    df_sum_plot_outfile = P.snip(sum_outfile, ".tsv") + ".png"
    df_norm_plot_outfile = P.snip(norm_outfile, ".tsv") + ".png"

    plotProfile(r_df_sum, df_sum_plot_outfile)
    plotProfile(r_df_norm, df_norm_plot_outfile)


@transform(createRepeatFastas,
           regex("iterative_mapping.dir/annotations.dir/tRNA.fa"),
           r"iterative_mapping.dir/annotations.dir/tRNA_sequences.tsv")
def collapsetRNAs(infile, outfile):
    ''' collapse tRNAs based on their 5' sequence and write out'''

    length = PARAMS['iterative_mapping_trna_length']

    P34.collapsetRNAs(infile, outfile, length)


@transform(collapsetRNAs,
           suffix(".tsv"),
           ".load")
def loadtRNAmap(infile, outfile):
    ''' load table mapping tRNA sequences to tRNA loci'''
    P.load(infile, outfile)


@follows(collapsetRNAs)
@transform(iterativeBowtieMapping,
           regex("iterative_mapping.dir/(\S+)_counts.tsv"),
           add_inputs(collapsetRNAs),
           r"iterative_mapping.dir/\1_counts_tRF_sequence.tsv")
def countReadsPertRF(infiles, outfile):
    ''' count reads per tRNA transcript fragment '''

    infile, collapsed_tRNAs = infiles

    length = PARAMS['iterative_mapping_trna_length']

    # we actually want the forward coverage file here
    infile = infile.replace("_counts.tsv", "_cov_forward.tsv")

    P34.countReadsPertRF(infile, outfile, collapsed_tRNAs, length)


@merge(countReadsPertRF,
       "iterative_mapping.dir/counts_tRF_sequence.tsv")
def mergeReadsPertRF(infiles, outfile):
    ''' merge the counts per tRF into a single table for DE analysis '''
    df = pd.DataFrame()

    for infile in infiles:
        sample = P.snip(os.path.basename(infile), "_counts_tRF_sequence.tsv")

        tmp_df = pd.read_table(infile, index_col="gene")
        df[sample] = tmp_df['counts']

    df.to_csv(outfile, sep="\t", index=True)

###############################################################################
# Iterative mapping - Quantification and DE - piRNAs
###############################################################################


@merge(iterativeBowtieMapping,
       "iterative_mapping.dir/counts_piRNA.tsv")
def mergeReadsPerpiRNA(infiles, outfile):
    ''' merge the counts per piRNA into a single table for DE analysis '''
    df = pd.DataFrame()

    for infile in infiles:
        sample = P.snip(os.path.basename(infile), "_counts.tsv")

        tmp_df = pd.read_table(infile, index_col=0, header=None)
        tmp_df = tmp_df.ix[tmp_df.index == "piRNA"]
        
        # reset index to transcript_id
        tmp_df.set_index([1], inplace=True)

        df[sample] = tmp_df.ix[:, 2]

    df.index.name = 'gene'
    df.to_csv(outfile, sep="\t", index=True)


###############################################################################
# Iterative mapping - Quantification - miRNAs
###############################################################################
# Quantification at three levels:
# 1. total counts per loci as per the piRNA
# 2. max counts per loci (mature miRNA)

@merge(iterativeBowtieMapping,
       "iterative_mapping.dir/counts_miRNA.tsv")
def mergeReadsPermiRNA(infiles, outfile):
    ''' merge the counts per miRNA into a single table for DE analysis '''
    df = pd.DataFrame()

    for infile in infiles:
        sample = P.snip(os.path.basename(infile), "_counts.tsv")

        tmp_df = pd.read_table(infile, index_col=0, header=None)
        tmp_df = tmp_df.ix[tmp_df.index == "miRNA"]
        #tmp_df = tmp_df.ix[tmp_df.index == "miRBase"]

        # reset index to transcript_id
        tmp_df.set_index([1], inplace=True)

        df[sample] = tmp_df.ix[:, 2]

    df.index.name = 'gene'
    df.to_csv(outfile, sep="\t", index=True)


@merge(iterativeBowtieMapping,
       "iterative_mapping.dir/counts_mature_miRNA.tsv")
def mergeReadsPerMaturemiRNA(infiles, outfile):
    ''' merge the counts per miRNA into a single table for DE analysis '''
    df = pd.DataFrame()

    for infile in infiles:
        sample = P.snip(os.path.basename(infile), "_counts.tsv")

        # we actually want the coverage on the forward strand
        infile = infile.replace("_counts.tsv", "_cov_forward.tsv")

        tmp_df = pd.read_table(infile, sep="\t", header=None,
                               usecols=[0, 1, 2], index_col=0)

        tmp_df = tmp_df.ix[tmp_df.index == "miRNA"]
        # tmp_df = tmp_df.ix[tmp_df.index == "miRBase"]

        df[sample] = [max(map(int, x.split(","))) for x in tmp_df.ix[:, 2]]

    df.set_index(tmp_df.ix[:, 1], inplace=True)
    df.index.name = "gene"

    df.to_csv(outfile, sep="\t", index=True)


###############################################################################
# Iterative Mapping - plot Clustering/Heatmaps
###############################################################################
@mkdir("iterative_mapping.dir/plots.dir")
@transform((mergeReadsPerMaturemiRNA,
            mergeReadsPerpiRNA,
            mergeReadsPertRF),
           regex("(\S+)/(\S+).tsv"),
           add_inputs("F1.design.tsv"),
           r"iterative_mapping.dir/plots.dir/\2_heatmap.png")
def plotHeatmapIterative(infiles, outfile):

    counts_inf, design_inf = infiles

    if "F1" in design_inf:
        short_names = True
    else:
        short_names = False

    P34.plotHeatmap(counts_inf, design_inf, outfile, short_names=short_names,
                    job_memory="2G", submit=True)


@mkdir("iterative_mapping.dir/plots.dir")
@transform((mergeReadsPerMaturemiRNA,
            mergeReadsPerpiRNA,
            mergeReadsPertRF),
           regex("(\S+)/(\S+).tsv"),
           add_inputs("F1.design.tsv"),
           r"iterative_mapping.dir/plots.dir/\2_correlation_heatmap.png")
def plotCorrelationHeatmapIterative(infiles, outfile):

    counts_inf, design_inf = infiles

    if "F1" in design_inf:
        short_names = True
    else:
        short_names = False
    print "\n\n\n\n\n\n\n", counts_inf, "\n\n\n\n\n\n"
    P34.plotCorrelationHeatmap(
        counts_inf, design_inf, outfile, short_names=short_names,
        job_memory="2G", submit=True)

###############################################################################
# Iterative Mapping - load counts to database
###############################################################################

@transform((mergeReadsPertRF,
           mergeReadsPerpiRNA,
           mergeReadsPermiRNA,
           mergeReadsPerMaturemiRNA),
           suffix(".tsv"),
           ".load")
def loadIterativeCounts(infile, outfile):
    ''' load the counts tables from the merged iterative mapping results'''
    P.load(infile, outfile)


###############################################################################
# Iterative Mapping - Data exploration
###############################################################################

@follows(mkdir("iterative_mapping.dir/plots.dir"))
@transform((mergeReadsPertRF,
           mergeReadsPerpiRNA,
           mergeReadsPermiRNA,
           mergeReadsPerMaturemiRNA),
           regex("iterative_mapping.dir/(\S+).tsv"),
           add_inputs("F1.design.tsv"),
           r"iterative_mapping.dir/plots.dir/\1.pca_PC1_PC2.png")
def plotPCAsAndDendoIterativeMapping(infiles, outfile):

    counts_inf, design_inf = infiles

    P34.plotPCAsAndDendo(counts_inf, design_inf, outfile, submit=True)


###############################################################################
# Iterative Mapping - Differential expression
###############################################################################

@follows(mkdir("iterative_mapping.dir/DESeq.dir"))
@transform((mergeReadsPertRF,
           mergeReadsPerpiRNA,
           mergeReadsPermiRNA,
           mergeReadsPerMaturemiRNA),
           regex("iterative_mapping.dir/(\S+).tsv"),
           add_inputs("F1.design.tsv"),
           r"iterative_mapping.dir/DESeq.dir/\1.deseq2_F1-Saline_F1-Dex_summary.tsv")
def runDESeq2IterativeMapping(infiles, outfile):

    counts_inf, design_inf = infiles
    outfile_pattern = P.snip(outfile, "deseq2_F1-Saline_F1-Dex_summary.tsv")

    statement = '''
    python %(scriptsdir)s/counts2table.py
    --tags-tsv-file=%(counts_inf)s
    --design-tsv-file=%(design_inf)s
    --output-filename-pattern=%(outfile_pattern)s
    --outfile=%(outfile)s
    --method=deseq2
    --fdr=0.1
    '''

    P.run()

###############################################################################
# Gene profiles
###############################################################################


@follows(mkdir("gene_profiles.dir"))
@files([(("%s.bam" % x.asFile(), "%s.gtf.gz" % y.asFile()),
         ("gene_profiles.dir/%s_%s.tsv.gz" % (x.asFile(), y.asFile())))
        for x, y in itertools.product(TRACKS, GENESETS)])
def buildGeneProfiles(infiles, outfile):

    to_cluster = True

    bamfile, gtffile = infiles
    track = P.snip(bamfile, '.bam')

    statement = '''python %(scriptsdir)s/bam2geneprofile.py
                      --output-filename-pattern="%(outfile)s.%%s"
                      --reporter=transcript
                      --method=geneprofile
                      --normalize-transcript=total-max
                      --normalize-profile=area
                      %(bamfile)s %(gtffile)s
                   > %(outfile)s
                '''
    P.run()


###############################################################################
# plots - sRNA species fractions
###############################################################################

@mkdir("plots.dir")
@originate(["plots.dir/sRNA_fractions.png"])
def plotsRNAFractions(outfile):
    dbh = connect()

    context_table = "mapping.context_stats"
    df = pd.read_sql_query(
        "select track,rRNA,tRNA,miRNA,total from %s" % context_table,
        dbh, index_col="track")

    piRNA_table = "rnaseqdiff.piRBase_rn5_feature_counts"
    piRNA_df = pd.read_sql_query("select * from %s" % piRNA_table,
                                 dbh, index_col="Geneid")
    piRNA_df = pd.DataFrame({"piRNA": piRNA_df.sum(0)})
    piRNA_df.index = [x.replace("_", "-") + ".bwa" for x in piRNA_df.index]

    def total2other(row):
        ''' obtain the remainder from total - all other rows'''
        return row["total"] - sum(row[[x for x in row.index if x != "total"]])

    def count2fraction(row):
        ''' obtain the fraction per annotation'''
        return [float(x)/row["total"] for x in row]

    df = pd.merge(df[["bwa" in x for x in df.index]], piRNA_df,
                  left_index=True, right_index=True)

    df['Other'] = df.apply(total2other, axis=1)
    fraction_df = df.apply(count2fraction, axis=1)

    fraction_df.to_csv(outfile.replace(".png", ".tsv"), sep="\t")

    df.drop("total", axis=1, inplace=True)

    df['sample'] = ["-".join(x.split(".")[0].split("-")[1:3])
                    for x in df.index]
    df['sample'] = [x.replace("Saline", "Veh") for x in df['sample']]
    df['generation'] = [x.split(".")[0].split("-")[0] for x in df.index]
    df['mapper'] = [x.split(".")[1] for x in df.index]
    df.index = [x.split(".")[0] for x in df.index]


    melt_df = pd.melt(df, id_vars=["sample", "generation", "mapper"])

    r_df = com.convert_to_r_dataframe(melt_df)

    plotFractions = R('''
    library(ggplot2)
    library(grid)
    function(df){

    df$variable = factor(df$variable, c("miRNA", "piRNA", "tRNA", "rRNA", "Other"))
    df = df[order(df$variable),]

    m = 30
    s = 25
    m_text = element_text(size=m)
    s_text = element_text(size=s)

    t = theme(
    aspect.ratio=1,
    axis.text.x = element_text(size=m, angle=90, hjust=1, vjust=0.5),
    axis.text.y = s_text,
    axis.title.x = m_text,
    axis.title.y = m_text,
    legend.title = s_text,
    legend.text = s_text,
    legend.key.size=unit(1, "cm"))

    p = ggplot(df, aes(x=as.factor(sample),
                       y=value,
                       fill=as.factor(variable))) +
    geom_bar(position="fill", stat="identity") +
    xlab("") + ylab("Fraction") +
    scale_fill_discrete(name="sRNA species") +
    theme_bw() + t

    ggsave("%(outfile)s", width=9, height=9)

    }''' % locals())

    plotFractions(r_df)


###############################################################################
# plots - length distributions
###############################################################################


@mkdir("plots.dir")
@originate("plots.dir/length_profiles.png")
def plotSizeProfiles(plotfile):
    ''' plot the size distribution for trimmed reads '''

    dbh = connect()

    def normalise(array):
        sum_array = sum(array)
        return [100*float(x)/sum_array for x in array]

    df = pd.DataFrame()

    for sample in ["%s_%s_%i" % (x, y, z) for x, y, z in itertools.product(
            ["F1", "F2"], ["Saline", "Dex"], [1, 2, 3, 4])]:

        tmp_df = pd.read_sql_query(
            '''SELECT * FROM
            readqc.trimmomatic_%s_fastqc_Sequence_Length_Distribution''' % sample,
            dbh)

        tmp_df = tmp_df[tmp_df["Length"] <= 38]
        tmp_df["norm"] = normalise(tmp_df["Count"])
        sample = sample.replace("Saline", "Veh")
        tmp_df["sample"] = sample
        tmp_df["Generation"] = sample.split("_")[0]
        tmp_df["Treatment"] = sample.split("_")[1]
        tmp_df["Replicate"] = sample.split("_")[2]

        df = pd.concat([df, tmp_df], axis=0)

    plot_lengths = R('''
    function(df){

    library(ggplot2)
    library(grid)
    library(cowplot)

    s = 30
    m = 35
    m_txt = element_text(size=m)
    s_txt = element_text(size=s)

    p = ggplot(df[df$Generation=="F1",],
               aes(x=as.numeric(as.character(Length)),
                   y=as.numeric(as.character(norm)),
                   colour=as.factor(Treatment),
                   group=as.factor(sample))) +
    geom_line(size=1) +
    scale_colour_discrete(name="") +
    ylab("Reads (%%)") +
    xlab("Length (bp)") +
    theme_cowplot() +
    scale_y_continuous(breaks=seq(0,12,2), limits=c(0,12)) +
    scale_x_continuous(breaks=seq(18,38,2), limits=c(18,38)) +
    scale_colour_manual(name="", values=c("#9e66ab", "#f9a65a")) +
    theme(
    aspect.ratio=1,
    axis.text.x=element_text(angle=90, size=m, vjust=0.5, hjust=1),
    axis.text.y=m_txt,
    axis.title.x=s_txt,
    axis.title.y=m_txt,
    legend.text=m_txt,
    legend.title=m_txt,
    legend.key.size=unit(1.5, "cm")) +
    guides(colour = guide_legend(override.aes = list(size=3)))
    geom_line()

    ggsave("%(plotfile)s", width=10, height=10)

    }''' % locals())

    df.index = range(0, len(df))
    df.to_csv(plotfile.replace(".png", ".tsv"), sep="\t")

    plot_lengths(com.convert_to_r_dataframe(df))


###############################################################################
# PCAs and Clustering
###############################################################################


@follows(mkdir("clustering.dir"))
@files([(("%s.counts.tsv.gz" % x.asFile(), "%s.design.tsv" % y.asFile()),
         ("clustering.dir/%s_%s.pca_PC1_PC2.png" % (x.asFile(), y.asFile())))
        for x, y in itertools.product(COUNTS, DESIGNS)])
def plotPCAsAndDendo(infiles, outfile):

    counts_inf, design_inf = infiles

    job_memory = "2G"

    P34.plotPCAsAndDendo(counts_inf, design_inf, outfile, submit=True)


@follows(mkdir("clustering.dir"))
@files([(("%s.counts.tsv.gz" % x.asFile(), "%s.design.tsv" % y.asFile()),
         ("clustering.dir/%s_%s_heatmap.png" % (x.asFile(), y.asFile())))
        for x, y in itertools.product(COUNTS, DESIGNS)])
def plotHeatmap(infiles, outfile):

    counts_inf, design_inf = infiles

    if "F1" in design_inf:
        short_names = True
    else:
        short_names = False

    P34.plotHeatmap(counts_inf, design_inf, outfile, short_names=short_names,
                    job_memory="2G", submit=True)


@follows(mkdir("clustering.dir"))
@files([(("%s.counts.tsv.gz" % x.asFile(), "%s.design.tsv" % y.asFile()),
         ("clustering.dir/%s_%s_correlation_heatmap.png" % (x.asFile(), y.asFile())))
        for x, y in itertools.product(COUNTS, DESIGNS)
        if y.asFile() == "F1" and "bwa" in x.asFile()])
def plotCorrelationHeatmap(infiles, outfile):

    counts_inf, design_inf = infiles

    if "F1" in design_inf:
        short_names = True
    else:
        short_names = False

    P34.plotCorrelationHeatmap(
        counts_inf, design_inf, outfile, short_names=short_names,
        job_memory="2G", submit=True)


###############################################################################
# Differential expression
###############################################################################

@follows(mkdir("DESeq.dir"))
@files([(("%s.counts.tsv.gz" % x.asFile(), "F1.design.tsv"),
        ("DESeq.dir/%s_deseq2_F1-Saline_F1-Dex_results.tsv" % x.asFile()))
        for x in COUNTS])
def runDESeq2(infiles, outfile):

    counts_infile, design_infile = infiles
    outfile_pattern = P.snip(outfile, "results.tsv")

    statement = '''
    python %(scriptsdir)s/counts2table.py
    --tags-tsv-file=%(counts_infile)s
    --design-tsv-file=%(design_infile)s
    --method=deseq2
    --output-filename-pattern=%(outfile_pattern)s
    --fdr=0.1 > %(outfile)s
    '''

    P.run()

###############################################################################
# Differential expression - power analysis
###############################################################################


@follows(mkdir("DESeq.dir"))
@merge((x.asFile() for x in COUNTS),
       "DESeq.dir/merged_counts.tsv")
def mergeCounts(infiles, outfile):
    ''' merge counts files and create spike ins'''

    df = pd.DataFrame()

    for inf in infiles:
        if "bwa" in inf and "tRNA" not in inf:
            tmp_df = pd.read_table(inf + ".counts.tsv.gz",
                                   sep="\t", index_col=0)
            df = pd.concat((tmp_df, df), axis=0)

    df.to_csv(outfile, sep="\t")


@transform(mergeCounts,
           suffix(".tsv"),
           "_plus_spike.tsv")
def addSpikeIns(infile, outfile):
    ''' add spike ins to merged counts '''

    statement = '''
    cat %(infile)s | python %(scriptsdir)s/counts2counts.py
    --design-tsv-file=F1.design.tsv --method="spike"
    --spike-type="row" --spike-maximum=100 --spike-minimum=100
    --spike-change-bin-min=-0.25 --spike-change-bin-max=2.25
    --spike-change-bin-width=0.5
    --spike-initial-bin-min=0 --spike-initial-bin-max=15
    --spike-initial-bin-width=2
    --spike-difference-method=abs_logfold --spike-iterations=1000
    --spike-output-method=append > %(outfile)s '''

    job_memory = "4G"
    P.run()


@transform(addSpikeIns,
           suffix(".tsv"),
           "_deseq.sentinel")
def SpikeInsDESeqAnalysis(infile, sentinel):
    ''' run DESeq2 +/- spike ins and plot MA and power '''

    outfile_prefix = P.snip(sentinel, ".sentinel")
    design = "F1.design.tsv"

    P34.spikeInDESeq2(infile, design, outfile_prefix,
                      submit=True, job_memory="2G")
    P.touch(sentinel)

###############################################################################
# load counts
###############################################################################

# this will load any counts tables in the working directory into the databse
# Jessy wants to have a play with the results

@files([("%s.counts.tsv.gz" % x.asFile(), "%s.counts.load" % x)
        for x in COUNTS])
def loadRawCounts(infile, outfile):
    ''' load counts table '''

    P.load(infile, outfile)

###############################################################################
# Meta-targets
###############################################################################


@follows(mkdir("iterative_mapping.dir"),
         mkdir("iterative_mapping.dir/annotations.dir"),
         mkdir("iterative_mapping.dir/DESeq.dir"),
         collapseFastq,
         createRepeatFastas,
         createCombinedRepeatsFastas,
         createFastasFromGTFs,
         createFastas,
         mergetRNAProfiles,
         plotPCAsAndDendoIterativeMapping,
         runDESeq2IterativeMapping,
         loadIterativeCounts,
         loadtRNAmap,
         plotHeatmapIterative,
         plotCorrelationHeatmapIterative)
def iterativeMapping():
        pass


@follows(SpikeInsDESeqAnalysis)
def spike():
    pass


@follows(plotPCAsAndDendo,
         plotHeatmap,
         plotsRNAFractions,
         plotSizeProfiles,
         plotCorrelationHeatmap)
def plot():
    pass


@follows(loadRawCounts,
         runDESeq2,
         plot,
         iterativeMapping,
         spike)
def full():
    pass


###############################################################################
# Reporting
###############################################################################

@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
